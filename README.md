# ICL-paper
### **In Context Learning** 

---

#### **Influence Factors for ICL**

**selection**

**1.What Makes Good In-Context Examples for GPT-3?** ACL2022 [paper](https://aclanthology.org/2022.deelio-1.10.pdf)

**2.Demystifying prompts in language models via perplexity estimation** ACL2023 [paper](https://aclanthology.org/2023.findings-emnlp.679.pdf)

**3.Diverse Demonstrations Improve In-context Compositional Generalization** ACL2023 [paper](https://aclanthology.org/2023.acl-long.78.pdf)

**4.Coverage-based Example Selection for In-Context Learning** ACL2023 [paper](https://aclanthology.org/2023.findings-emnlp.930.pdf)

**5.Compositional Exemplars for In-context Learning** ICML2023 [paper](https://arxiv.org/pdf/2302.05698)

**6.In-Context Learning with Iterative Demonstration Selection** ICLR2024 [paper](https://openreview.net/pdf?id=g5Iqg4BwsF)

**7.C-ICL: Contrastive In-context Learning for Information Extraction** 202406 [paper](https://arxiv.org/pdf/2402.11254)

**8.IDEAL: Influence-Driven Selective Annotations Empower In-Context Learners in Large Language Models** ICLR2024 [paper](https://openreview.net/pdf?id=Spp2i1hKwV)

**9.Selective Annotation Makes Language Models Better Few-Shot Learners** ICLR2024 [paper](https://openreview.net/pdf?id=Spp2i1hKwV)

**10.Which Examples to Annotate for In-Context Learning? Towards Effective and Efficient Selection** 202310 [paper](https://arxiv.org/pdf/2310.20046)

**11.Unifying Demonstration Selection and Compression for In-Context Learning** 202406 [paper](https://arxiv.org/pdf/2405.17062)

**12.GistScore: Learning Better Representations for In-Context Example Selection with Gist Bottlenecks** ICML2024 [paper](https://openreview.net/pdf?id=WCVC5wGZyz)

**13.MICL: Improving In-Context Learning through Multiple-Label Words in Demonstration** 202408 [paper](https://arxiv.org/pdf/2406.10908)

**14.In-Context Principle Learning from Mistakes** ICML2024 [paper](https://openreview.net/pdf?id=PAPY0cAB3C)

**15.Representative Demonstration Selection for In-Context Learning with Two-Stage Determinantal Point Process** ACL2023 [paper](https://aclanthology.org/2023.emnlp-main.331.pdf)

**16.Large Language Models Are Latent Variable Models: Explaining and Finding Good Demonstrations for In-Context Learning** NeurIPS2023 [paper](https://proceedings.neurips.cc/paper_files/paper/2023/file/3255a7554605a88800f4e120b3a929e1-Paper-Conference.pdf)

**17.Learning to Retrieve In-Context Examples for Large Language Models** ACL2024 [paper](https://aclanthology.org/2024.eacl-long.105.pdf)

**18."In-Context Learning" or: How I learned to stop worrying and love "Applied Information Retrieval"** SIGIR2024 [paper](https://arxiv.org/pdf/2405.01116)

**19.$Se^2$: Sequential Example Selection for In-Context Learning** ACL2024 [paper](https://aclanthology.org/2024.findings-acl.312.pdf)

**20.RetICL: Sequential Retrieval of In-Context Examples with Reinforcement Learning** 202404 [paper](https://arxiv.org/pdf/2305.14502)

**21.Self-Generated In-Context Learning: Leveraging Auto-regressive Language Models as a Demonstration Generator** NAACL2022 [paper](https://arxiv.org/pdf/2206.08082)

**22.An Information-theoretic Approach to Prompt Engineering Without Ground Truth Labels** ACL2022 [paper](https://aclanthology.org/2022.acl-long.60.pdf)

**23.Active example selection for in-context learning** ACL2022 [paper](https://aclanthology.org/2022.emnlp-main.622.pdf)

**24.Towards Robust In-Context Learning for Machine Translation with Large Language Models** ACL2024 [paper](https://aclanthology.org/2024.lrec-main.1444.pdf)

**25.Complementary Explanations for Effective In-Context Learning** ACL2023 [paper](https://aclanthology.org/2023.findings-acl.273.pdf)


**ordering**

**1.Calibrate Before Use: Improving Few-Shot Performance of Language Models** ICML2021 [paper](https://arxiv.org/pdf/2102.09690)

**2.What Makes Good In-Context Examples for GPT-3?** ACL2022 [paper](https://aclanthology.org/2022.deelio-1.10.pdf)

**3.Fantastically Ordered Prompts and Where to Find Them: Overcoming Few- Shot Prompt Order Sensitivity** ACL2022 [paper](https://aclanthology.org/2022.acl-long.556.pdf)

**4.Let's Learn Step by Step: Enhancing In-Context Learning Ability with Curriculum Learning** 202406 [paper](https://arxiv.org/pdf/2402.10738)

**5.Batch-ICL: Effective, Efficient, and Order-Agnostic In-Context Learning** ACL2024 [paper](https://aclanthology.org/2024.findings-acl.638.pdf)



**format**

**1.Cross-Task Generalization via Natural Language Crowdsourcing Instructions** ACL2022 [paper](https://aclanthology.org/2022.acl-long.244.pdf)

**2.Super-Natural Instructions: Generalization via Declarativ Instructions on 1600+ NLP Tasks** ACL2022 [paper](https://aclanthology.org/2022.emnlp-main.340.pdf)

**3.In-context Vectors: Making In Context Learning More Effective and Controllable Through Latent Space Steering** ICML2024 [paper](https://openreview.net/attachment?id=dJTChKgv3a&name=pdf)



**CoT**

**1.Chain-of-Thought Prompting Elicits Reasoning in Large Language Models** NeurIPS2022 [paper](https://openreview.net/pdf?id=_VjQlMeSB_J)

**2.Self-Consistency Improves Chain of Thought Reasoning in Language Models** ICLR2023 [paper](https://openreview.net/pdf?id=1PL1NIMMrw)

**3.Automatic Chain of Thought Prompting in Large Language Models** ICLR2023 [paper](https://openreview.net/pdf?id=5NTt8GFjUHkr)

**4.Large Language Models Are Reasoning Teachers** ACL2023 [paper](https://aclanthology.org/2023.acl-long.830.pdf)


---

#### **mechanism of ICL**

**1.Transformers learn in-context by gradient descent** ICML2023 [paper](https://arxiv.org/pdf/2212.07677)

**2.Why Can GPT Learn In-Context? Language Models Implicitly Perform Gradient Descent as Meta-Optimizers** ACL2023 [paper](https://arxiv.org/pdf/2212.10559)

**3.Position Do pretrained Transformers Learn In-Context by Gradient Descent** ICML2024 [paper](https://openreview.net/pdf?id=WsawczEqO6)

**4.Rethinking the Role of Demonstrations: What Makes In-Context Learning Work** ACL2022 [paper](https://aclanthology.org/2022.emnlp-main.759.pdf)

**5.In-context Learning and Induction Heads** 202209 [paper](https://arxiv.org/pdf/2209.11895)

**6.An information flow perspective for understanding in-context learning** ACL2023 [paper](https://aclanthology.org/2023.emnlp-main.609.pdf)

**7.Transformers learn to implement preconditioned gradient descent for in-context learning** NeurIPS2023 [paper](https://openreview.net/pdf?id=LziniAXEI9)

**8.What learning algorithm is in-context learning? Investigations with linear models** ICLR2023 [paper](https://openreview.net/pdf?id=0g0X4H8yN4I)

**9.Transformers as statisticians: Provable in-context learning with in-context algorithm selection** ICML2023 [paper](https://openreview.net/pdf?id=vlCG5HKEkI)

**10.Data Distributional Properties Drive Emergent In-Context Learning in Transformers** NeurIPS2022 [paper](https://arxiv.org/pdf/2205.05055)

**11.Dual operating modes of in context learning** ICLR2024 [paper](https://openreview.net/pdf?id=5H4nJIGqmK)

**12.An Explanation of In-context Learning as Implicit Bayesian Inference** ICLR2022 [paper](https://arxiv.org/pdf/2111.02080)

**13.Transformers are Minimax Optimal Nonparametric In-Context Learners** ICML2024 [paper](https://openreview.net/pdf?id=WZyWbtu39H)

**14.Transformers Learn Temporal Difference Methods** ICML2024[paper](https://openreview.net/pdf?id=mEqddgqf5w)

**15. Probing the Decision Boundaries of In-context Learning in Large Language Models** ICML2024 [paper](https://openreview.net/pdf?id=t90UB9wvUZ)

**16.In-Context Language Learning Architectures and Algorithms** ICML2024 [paper](https://openreview.net/pdf?id=3Z9CRr5srL)

---

#### application

**1.Crafting a Good Prompt or Providing Exemplary Dialogues? A Study of In-Context Learning for Persona-based Dialogue Generation** 202402 [paper](https://arxiv.org/pdf/2402.09954)

**2.Self-Augmented In-Context Learning for Unsupervised Word Translation** ACL2024 [paper](https://aclanthology.org/2024.acl-short.67.pdf)

---

#### **Survey**

**1.A Survey on In-context Learning** 202406 [paper]([2301.00234 (arxiv.org)](https://arxiv.org/pdf/2301.00234))

**2.In-context Learning with Retrieved Demonstrations for Language Models: A Survey** 202403 [paper]([2401.11624 (arxiv.org)](https://arxiv.org/pdf/2401.11624))
