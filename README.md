# ICL-paper
### **In Context Learning** 

---

#### **Influence Factors for ICL**

**selection**

**1.What Makes Good In-Context Examples for GPT-3? **[paper]

**2.Demystifying prompts in language models via perplexity estimation **[paper]

**3.Diverse Demonstrations Improve In-context Compositional Generalization **[paper]

**4.Coverage-based Example Selection for In-Context Learning **[paper]

**5.Compositional Exemplars for In-context Learning** [paper]

**6.In-Context Learning with Iterative Demonstration Selection** [paper]

**7.C-ICL: Contrastive In-context Learning for Information Extraction** [paper]

**8.IDEAL: Influence-Driven Selective Annotations Empower In-Context Learners in Large Language Models **[paper]

**9.Selective Annotation Makes Language Models Better Few-Shot Learners **[paper]

**10.Which Examples to Annotate for In-Context Learning? Towards Effective and Efficient Selection** [paper]

**11.Unifying Demonstration Selection and Compression for In-Context Learning **[paper]

**12.GistScore: Learning Better Representations for In-Context Example Selection with Gist Bottlenecks **[paper]

**13.MICL: Improving In-Context Learning through Multiple-Label Words in Demonstration** [paper]

**14.In-Context Principle Learning from Mistakes** [paper]

**15.Representative Demonstration Selection for In-Context Learning with Two-Stage Determinantal Point Process **[paper]

**16.Large Language Models Are Latent Variable Models: Explaining and Finding Good Demonstrations for In-Context Learning** [paper]

**17.Learning to Retrieve In-Context Examples for Large Language Models **[paper]

**18."In-Context Learning" or: How I learned to stop worrying and love "Applied Information Retrieval" **[paper]

**19.$Se^2$: Sequential Example Selection for In-Context Learning **[paper]

**20.RetICL: Sequential Retrieval of In-Context Examples with Reinforcement Learning **[paper]

**21.Self-Generated In-Context Learning: Leveraging Auto-regressive Language Models as a Demonstration Generator **[paper]

**22.An Information-theoretic Approach to Prompt Engineering Without Ground Truth Labels** [paper]

**23.Active example selection for in-context learning** [paper]

**24.Towards Robust In-Context Learning for Machine Translation with Large Language Models** ACL2024 [paper]([2024.lrec-main.1444.pdf (aclanthology.org)](https://aclanthology.org/2024.lrec-main.1444.pdf))



**ordering**

**1.Calibrate Before Use: Improving Few-Shot Performance of Language Models** ICML2021 [paper]([2102.09690 (arxiv.org)](https://arxiv.org/pdf/2102.09690))

**2.What Makes Good In-Context Examples for GPT-3? **[paper]

**3.Fantastically Ordered Prompts and Where to Find Them: Overcoming Few- Shot Prompt Order Sensitivity** [paper]

**4.Let's Learn Step by Step: Enhancing In-Context Learning Ability with Curriculum Learning **[paper]

**5.Fantastically ordered prompts and where to find them: Overcoming few-shot prompt order sensitivity** ACL2022 [paper]([arxiv.org/pdf/2104.08786](https://arxiv.org/pdf/2104.08786))

**6.Batch-ICL: Effective, Efficient, and Order-Agnostic In-Context Learning** ACL2024 [paper]([aclanthology.org/2024.findings-acl.638.pdf](https://aclanthology.org/2024.findings-acl.638.pdf))



**format**

**1.Cross-Task Generalization via Natural Language Crowdsourcing Instructions **[paper]

**2.Super-Natural Instructions: Generalization via Declarativ Instructions on 1600+ NLP Tasks **[paper]

**3.In-context Vectors: Making In Context Learning More Effective and Controllable Through Latent Space Steering** [paper]



**CoT**

**1.Chain-of-Thought Prompting Elicits Reasoning in Large Language Models **[paper]

**2.Self-Consistency Improves Chain of Thought Reasoning in Language Models **[paper]

**3.Automatic Chain of Thought Prompting in Large Language Models** [paper]

**4.Large Language Models Are Reasoning Teachers **[paper]

**5.Chain of Thought Prompting Elicits Reasoning in Large Language Models** [paper]

---

#### **mechanism of ICL**

**1.Transformers learn in-context by gradient descent** [paper]

**2.Why Can GPT Learn In-Context? Language Models Implicitly Perform Gradient Descent as Meta-Optimizers **[paper]

**3.Position Do pretrained Transformers Learn In-Context by Gradient Descent** [paper]

**4.Rethinking the Role of Demonstrations: What Makes In-Context Learning Work **[paper]

**5.In-context Learning and Induction Heads** [paper]

**6.An information flow perspective for understanding in-context learning** [paper]

**7.Transformers learn to implement preconditioned gradient descent for in-context learning** [paper]

**8.What learning algorithm is in-context learning? Investigations with linear models **[paper]

**9.Transformers as statisticians: Provable in-context learning with in-context algorithm selection** [paper]

**10.Data Distributional Properties Drive Emergent In-Context Learning in Transformers **[paper]

**11.dual operating modes of in context learning** [paper]

**12.An Explanation of In-context Learning as Implicit Bayesian Inference **[paper]

**13.Position Do pretrained Transformers Learn In-Context by Gradient Descent **[paper]

**14.Transformers are Minimax Optimal Nonparametric In-Context Learners** [paper]

**15.Transformers Learn TemporalDifference Methods **[paper]

**16.Probing the Decision Boundaries of In-context **[paper]

**17.In-Context Language Learning Architectures and Algorithms **[paper]

---

#### application

**1.Crafting a Good Prompt or Providing Exemplary Dialogues? A Study of In-Context Learning for Persona-based Dialogue Generation ** 202402 [paper]([arxiv.org/pdf/2402.09954](https://arxiv.org/pdf/2402.09954))

**2.Self-Augmented In-Context Learning for Unsupervised Word Translation** ACL2024 [paper]([aclanthology.org/2024.acl-short.67.pdf](https://aclanthology.org/2024.acl-short.67.pdf))

---

#### **Survey**

**1.A Survey on In-context Learning **202406 [paper]([2301.00234 (arxiv.org)](https://arxiv.org/pdf/2301.00234))

**2.In-context Learning with Retrieved Demonstrations for Language Models: A Survey ** 202403 [paper]([2401.11624 (arxiv.org)](https://arxiv.org/pdf/2401.11624))
